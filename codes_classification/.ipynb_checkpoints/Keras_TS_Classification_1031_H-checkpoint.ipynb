{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pending-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "from pandas.core.common import flatten\n",
    "\n",
    "\n",
    "import keras\n",
    "import tensorflow\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "uniform-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "alarm = '1031_H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "harmful-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(alarm_name):\n",
    "    \"\"\" Returns a dataset of all valve values and specific alarm column.\"\"\"\n",
    "    \n",
    "    dfs = {file.split(\"_\")[-4]: pd.read_pickle(file) for file in \\\n",
    "           glob.glob(\"S:\\SRH\\BDBA_Sem_2\\Case_study_1\\data\\*.pkl\")}\n",
    "    \n",
    "    dfs_sorted = dict(sorted(dfs.items()))\n",
    "    df_single = pd.concat(dfs_sorted, axis=0)\n",
    "    \n",
    "    alarms = ['1031_H', '1031_L', '1034_H', '1034_L', '1037_H', '1037_L']\n",
    "    alarms.remove(alarm_name)\n",
    "    \n",
    "    df_alarm = df_single.drop(alarms, axis=1)\n",
    "    df_alarm.fillna(0, inplace=True)\n",
    "    \n",
    "    return df_alarm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecological-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alarm = create_df(alarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boring-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_sequence(data, past_seq_len, future_window):\n",
    "    \"\"\" Creates a small input sequence of a given seq length and \n",
    "        returns two numpy arrays as input and output sequence\n",
    "        \n",
    "        Args:\n",
    "        data: input dataframe\n",
    "        past_seq_len: integer number\n",
    "        future_window: integer number\n",
    "\n",
    "        \"\"\"\n",
    "    target_df = data.iloc[:,-1]\n",
    "    input_x = []\n",
    "    output_y = []\n",
    "    for i in range(len(data) - past_seq_len -1):\n",
    "        ins = data.iloc[i:(i+past_seq_len), 0:data.shape[1]-1]\n",
    "        ots = np.where((target_df.iloc[(i+past_seq_len):(i+past_seq_len+future_window)]>0).any(), 1, 0)\n",
    "        input_x.append(ins)\n",
    "        output_y.append(ots)\n",
    "    in_array = np.array(input_x).astype(np.float32)\n",
    "    out_array = np.array(output_y).astype(np.float32)\n",
    "    return in_array, out_array.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "infinite-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_df(df, val_pct):\n",
    "    \"\"\" Creates train, validation and test set after applying normalisation of all feature cols\n",
    "    Args:\n",
    "    df: dataframe object\n",
    "    val_pct: percentage size of validation plus test size (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    val_data_size = round(df.shape[0] * val_pct)\n",
    "    test_data_size = round(val_data_size * 0.1)\n",
    "    \n",
    "    train_data = df[:-val_data_size]\n",
    "    val_data = df[-val_data_size:-test_data_size]\n",
    "    test_data = df[-test_data_size:]\n",
    "    \n",
    "    # Scaling the data\n",
    "    scalar = MinMaxScaler()\n",
    "    scalar.fit(train_data.iloc[:,:-1])\n",
    "    # save the scaler\n",
    "    dump(scalar, open('model_objects\\scaler_cls_'+alarm+'.pkl', 'wb'))\n",
    "    \n",
    "    train_scaled = scalar.transform(train_data.iloc[:,:-1]) \n",
    "    val_scaled = scalar.transform(val_data.iloc[:,:-1])\n",
    "    test_scaled = scalar.transform(test_data.iloc[:,:-1])\n",
    "    \n",
    "    df_train = pd.DataFrame(train_scaled)\n",
    "    df_train['alarm'] = train_data.iloc[:,-1].values\n",
    "    df_val = pd.DataFrame(val_scaled)\n",
    "    df_val['alarm'] = val_data.iloc[:,-1].values\n",
    "    df_test = pd.DataFrame(test_scaled)\n",
    "    df_test['alarm'] = test_data.iloc[:,-1].values\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "immune-village",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_objects\\\\scaler_cls_1031_H.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7cc17d60090a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessed_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_alarm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-93fd696347ad>\u001b[0m in \u001b[0;36mpreprocessed_df\u001b[1;34m(df, val_pct)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mscalar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# save the scaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_objects\\scaler_cls_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0malarm\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtrain_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscalar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_objects\\\\scaler_cls_1031_H.pkl'"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test = preprocessed_df(df_alarm, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the window size\n",
    "n_steps = 15\n",
    "future_window = 20\n",
    "# split into samples\n",
    "X_train, y_train = input_sequence(df_train, n_steps, future_window)\n",
    "X_val, y_val = input_sequence(df_val, n_steps, future_window)\n",
    "X_test, y_test = input_sequence(df_test, n_steps, future_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train.shape: {X_train.shape},y_train.shape: {y_train.shape}\\n\"\n",
    "      f\"X_val.shape: {X_val.shape}, y_val.shape: {y_val.shape}\\n\"\n",
    "      f\"X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(100, kernel_initializer='he_normal',input_shape=(X_train.shape[1], X_train.shape[2]),return_sequences=False))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=\"Adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# configure early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "# Visualise model\n",
    "logdir = \"logs/ts_classification_\"+alarm+\"/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "checkpoint_filepath = './tmp'+alarm+'/checkpoint'\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "# fit the model\n",
    "model_history = model.fit(X_train, y_train, epochs=100, \n",
    "                          batch_size=32, verbose=2, \n",
    "                          validation_data=(X_val, y_val), \n",
    "                          callbacks=[es, tensorboard, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "realistic-reward",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-648d41023381>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loads the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./tmp'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0malarm\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/checkpoint'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Loads the weights\n",
    "model.load_weights('./tmp'+alarm+'/checkpoint')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Test accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-awareness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-single",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-homeless",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "regular-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    \"\"\" Return model objective using keras tuner for hyperparameters\"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "    model.add(LSTM(hp_units, activation='relu', kernel_initializer='he_normal', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(hp_units, kernel_initializer='he_normal', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Tune the learning rate for the optimizer \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate)\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "monetary-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "# Visualise model\n",
    "logdir = \"logs/ts_classification_tuned\"+alarm+\"/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "checkpoint_filepath = './tmp_tuned_'+alarm+'/checkpoint'\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "#define a callback to clear the training outputs at the end of every training step\n",
    "class ClearTrainingOutput(tensorflow.keras.callbacks.Callback):\n",
    "      def on_train_end(*args, **kwargs):\n",
    "            clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "changed-impression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 64 Complete [00h 59m 46s]\n",
      "val_accuracy: 0.9405169486999512\n",
      "\n",
      "Best val_accuracy So Far: 0.956244170665741\n",
      "Total elapsed time: 05h 52m 56s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_accuracy', \n",
    "                     max_epochs = 30,\n",
    "                     factor = 3,\n",
    "                     directory = logdir,\n",
    "                     project_name = 'ts_classification')\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=25, \n",
    "                          batch_size=32, verbose=2, \n",
    "                          validation_data=(X_val, y_val), \n",
    "                          callbacks=[es, tensorboard, model_checkpoint, ClearTrainingOutput()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
